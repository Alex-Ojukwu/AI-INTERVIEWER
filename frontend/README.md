# AI Virtual Interview Assistant - Frontend

Next.js frontend for the AI Virtual Interview Assistant application.

## Features

- **Real-time Video**: Webcam feed with live emotion detection
- **Audio Recording**: Record and transcribe interview answers
- **Avatar Display**: Talking avatar interviewer
- **Emotion Visualization**: Real-time expression meter
- **Responsive UI**: Modern, clean interface

## Tech Stack

- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: CSS (custom)
- **Real-time**: WebSockets

## Setup

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment

Copy `.env.local.example` to `.env.local`:

```bash
cp .env.local.example .env.local
```

Edit `.env.local`:

```
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_WS_URL=ws://localhost:8000
```

### 3. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000/interview](http://localhost:3000/interview)

## Project Structure

```
frontend/
├── app/
│   ├── interview/
│   │   ├── page.tsx              # Main interview interface
│   │   ├── components/           # UI components
│   │   │   ├── WebcamFeed.tsx    # Webcam capture
│   │   │   ├── AudioRecorder.tsx # Audio recording
│   │   │   ├── AvatarPlayer.tsx  # Avatar video player
│   │   │   ├── ExpressionMeter.tsx # Emotion display
│   │   │   ├── ChatBox.tsx       # Question display
│   │   │   └── Loader.tsx        # Loading indicator
│   │   └── styles.css            # Component styles
│   ├── layout.tsx                # Root layout
│   └── globals.css               # Global styles
├── lib/
│   ├── api.ts                    # API client functions
│   ├── avatar.ts                 # Avatar API helpers
│   ├── recorder.ts               # Audio recording utils
│   └── ws.ts                     # WebSocket client
└── public/
    └── assets/                   # Static assets
```

## Components

### WebcamFeed

Captures live video from webcam and sends frames for emotion analysis.

### AudioRecorder

Records user's voice answers and sends to Whisper API for transcription.

### AvatarPlayer

Displays talking avatar video generated by D-ID API.

### ExpressionMeter

Visualizes detected emotions in real-time with progress bars.

### ChatBox

Displays current interview question from AI.

## API Integration

All backend communication is handled through functions in `lib/api.ts`:

- `startInterview()` - Begin interview session
- `submitAnswer()` - Submit transcribed answer
- `analyzeEmotion()` - Send frame for emotion detection
- `transcribeAudio()` - Convert speech to text

## WebSocket

Real-time features use WebSocket connection (`lib/ws.ts`):

- Video frame streaming
- Emotion data updates
- Audio chunk transmission

## Browser Requirements

- Modern browser with WebRTC support
- Camera and microphone permissions
- JavaScript enabled

## Development

### Type Checking

```bash
npm run type-check
```

### Linting

```bash
npm run lint
```

### Build for Production

```bash
npm run build
npm start
```

## Notes

- Requires backend API running on port 8000
- Camera and microphone access must be granted
- Optimized for desktop browsers (Chrome, Firefox, Safari, Edge)

## License

Final Year Project - 2024/2025
